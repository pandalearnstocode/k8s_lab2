# Fast API Microservice architecture

## List of services

* __Client:__ Here we are going use a streamlit app, in a real world scenario it will be replaced by a real frontend app. The objective of this application is to trigger communication between other services and see that they interaction is working fine or not.
* __Application:__ All the project data, authentication & authorization level workflow will be managed in this layer. This will be connect to a project db & other tables in the db associated with the project meta data.
* __ML:__ This will be a service with all the heavy lifting related task like more training & serving etc. This needs to be connected with the celery worker, rabbit mq and flower.
* __Data:__ This is the service using which any data fetching and aggregation will happen from the db. redis needs to be integrated with this so that the same aggregated data is not pulled multiple times. It should be stored in a cached memory for certain time.
* __Reporting:__ Any data generated from the model will go through this layer to be reshaped in the desired from.
* __DB:__ Here for simplification we are going to create one db, but in real life scenario the project data db and raw data db will be different. the config needs to be adjusted according to that.
* __DB Admin:__ Same as above.
* __Prometheus:__ this will be used to monitor multiple services running in the cluster and gather all the metrics. Note, in case of ml layer some customs metrics needs to be fetched. 
* __Grafana:__ Use Loki to monitor all the logs generated by all the components. 
* __Utility:__ Common API services to help common tasks in different layers/services.
* __Redis Cache__
* __Celery__
* __Flower__
* __Rabbit MQ__

## __Other important components__

* __ML/Data/Utils library__
* __Ingress controller__
* __CI/CD__


### Workflow:

1. user enters user email and password from client
2. call reaches application service, it checks in project db the user id is present or not, if yes sends valid user.
3. client gets the response and unlocks further steps


4. as next step user wants to get data for a country, he/she selects country from drop down. click submit. call goes to data layer.
5. data layer fetch the data from raw data db, sends it back to client as json.

docker exec -it d7e6eb3cb69a psql -U pguser dwd


6. by looking at the data user wants to aggregate the data data, he/she selects aggregation level and send it to utils layer.
7. util layer apply aggregation to the data using some function from utils library and return the data to the client.
8. after the aggregate data is received the sends the data t ml layer to run some model.
9.  ml layer takes the data and put it in a queue which will be sent to a celery working using rabbit mq. once the jobs are running this status will be visible in flower. the function to run this workload will come from ml library.
10. once this result is generated it will be stored in a redis cache so that if there is a same call with the same payload this should not run twice. it should show result from the cache in memory storage. if the in memory storage has expired it should call the db to fetch the information.
11. now, once client has to post modelling data, it may want to report a image for fit chart, in that case the client will call the reporting layer.
12. reporting layer will get all the data and will create a fit chart to show in the ui. it will send this data back to client.
13. Client will show the data using charting library. 

* https://realpython.com/introduction-to-flask-part-2-creating-a-login-page/